# 注意力机制

对于一个测试输入，为了预测这个输入对应的输出，一个想法是：模型应该更关注那些和测试输入更近的训练输入。

所以一个比较简单的算法是选择某种加权方式，输出训练输出的加权平均数，而权重取决于测试输入和训练输入的相似程度。

在这样的问题中，训练输入（特征）一般被称为：键，训练输入的输出（标签）一本被称为：值。

## Nadaraya-Waston 核回归

考虑一个函数 $K(u)$，可以将距离 $u$ 和权值 $K(u)$ 进行映射，更小的距离对应了更大的权值，最后再将所有 “值” 的权值标准化即可。其作用类似于图像处理中, 卷积时的卷积核, 所以又称核函数.

高斯核, 即高斯分布 (正态分布) 函数作为权值的核函数的效果优秀：

$$
K(u) = \frac {e^{-\frac {u^2}2}}{\sqrt {2\pi}}
$$

发现在权值标准化操作后，整套权值计算的过程可以被认为是关于 $-\frac {u^2}2$ 的 `softmax` 操作。也就是对 $-\frac {(x - x_i)^2}2$ 的 `softmax` 操作。

这种给更近的 “键-值” 更高的权值的机制，也就是注意力机制。

但是，高斯核的宽度不需要固定，它可以被一个参数调整：$u = w(x - x_i)$. 而 $w$ 可以通过训练得到。

## 注意力分数

计算加权平均数的权值，称作注意力权重，而权重的依据便是注意力分数计算的。前面的例子中，输出是标量，所以注意力函数可以是简单地二者相加。但是在更多的情况中，需要对两个张量计算后映射到一个标量，才能执行 `softmax` 运算。

$$
a(\bold q, \bold k_i)
$$

分数函数 $a$, 询问 $\bold q$, 键 $\bold k$.

## 掩蔽 softmax

汇聚层: 输出是滑动窗口内的最大值或平均值, 将较大规模的输入汇聚到更小规模的输出.

发现在高斯核处理后, 距离较远的键的权值会变得很小，而且在逻辑上，很远的键对应的值也不具有参考价值，反而会扰乱输出，所以不妨直接删除这些点的影响，让原本全部样本参与加权平均变成部分样本加权平均，可以认为是一种汇聚。

## 加性注意力

为了更好衡量两个张量的相关性，注意力分数函数也可以用带参数的网络来描述：

$$
a(\bold q, \bold k) = \bold w_v^T \tanh(\bold{W_qq + W_kk})
$$

其中 $\tanh$ 的自变量是一个张量, 表示对这个张量每个元素都做 $\tanh$, 然后用 $\bold w_v^T$ 将结果张量还原成标量。三个参数张量都是可以训练调整的。

## 缩放点积注意力

这种注意力分数的计算不存在可调参数，选择向量的点积作为函数值，为了将随机向量点积的方差修正到向量的分量的方差，还需要除以维数的平方根：

$$
a(\bold q, \bold k) = \frac {\bold {q^Tk}}{\sqrt d}
$$

## 编码和解码

长度可变的序列作为输入和输出，需要将序列转化为固定长度的隐藏状态，然后将固定长度的输出转化回序列。这要求存在编码器和解码器，用循环神经网络来实现。

为了将序列编码为向量，每个序列的元素首先要被编码为特征向量，嵌入层的作用就是将数据转换为向量 $\bold x_i$。然后将向量传入循环神经网络，将上下文信息编码到向量中：

$$
\bold h_i = f(\bold x_i, \bold h_{i - 1})
$$

然后将隐层状态处理成一个上下文变量 $\bold c$，存储整个输入序列的信息：

$$
\bold c = q(\bold h_1, ..., h_i, ...)
$$

解码时，假设现在需要确定输出序列的第 $i$ 个元素，可以通过另一个循环神经网络来处理：

$$
\bold s_i = g(y_{i - 1}, \bold c, \bold s_{i - 1})
$$

$s$ 也存储了输出时的上下文信息，为了预测第 $i$ 个元素，需要计算在当前上下文环境下，下一个输出的条件概率即可。这也是一个 `softmax` 运算。

## Bahdanau 注意力

在编码器-解码器架构中引入注意力机制，使得输出时每个时刻注意的上下文 $c$ 不同。在解码器中，预测第 $i$ 个输出时, 查询是 $s_{i - 1}$，而键-值则都是 $h$ 向量. 这个过程的注意力分数函数可以采用前面提到的加性注意力函数. 而注意力加权和则可以当作这个时刻注意到的上下文 $c_i$.

可以独立训练多个注意力头，并行地计算各自的注意力加权平均，然后经过一层线性组合得到最终的输出。这样可以注意到不同的信息，得到更全面的上下文。性能上的优势是对并行更友好。

## 自注意力

将每一个 $x$ 作为查询, 各自获得一个注意力输出 $y$, 由于键和值也都是 $x$, 所以这种机制称为 "自注意力".

自注意力的作用是将输入的不同元素的信息互相传递，是一种不同于循环神经网络的编码的方式。性能上，自注意力的并行度是 $O(n)$, 但是复杂度是 $O(n^2d)$, 对于长序列的性能代价更高。

但是和循环神经网络不同的是，自注意力没有利用序列的位置信息，为了考虑元素在序列中的位置，需要在元素上记录它的位置信息。这种操作被称为位置编码.

假设输入向量构成的矩阵是 $\bold X$, 那么在其上叠加一个位置信息 $P$:

$$
\begin{aligned}
p_{i, 2j} &= \sin{\frac i{10000^{2j/d}}}\\
p_{i, 2j + 1} &= \cos{\frac i{10000^{2j/d}}}
\end{aligned}
$$

越靠后的列频率越低，频率呈指数降低，类似于二进制或其它进制中，每一位数字变化的频率呈指数降低。相当于将每个位置转化为一个 $\frac d2$ 位的某进制数，每个位由两个值指示的相位角决定。由于指示位置的是 $\frac d2$ 个角度，所以通过做差可以得到相对位置，线性变化也可以找到某个相对距离的绝对位置。

## Transformer

基于编码器-解码器架构，和循环神经网路不同，Transformer 的编解码完全没有用到循环神经网路，而是用自注意力模块传递隐藏状态的信息。

除了自注意力模块，还存在 "基于位置的前馈网络", 对于每个向量，将它们传入同一个多层感知机，然后输出形状相同的向量。这样操作的目的是增强和变换特征，特点是每个向量是独立的，不会和其他向量产生影响。

多头的自注意力模块和前馈网络组成了 Transformer 编码器的一个层。这样的层存在若干个。

对于解码器，需要顺序生成序列的任务，需要用掩码来取消正在生成的向量后面的向量对这个向量的影响。所以解码器的层的多头自注意力模块是存在掩码的。除此之外，还存在以编码器为键-值的另一个多头注意力模块，还有最后的前馈网络。

## Encoder-Decoder 

大语言模型 (LLM) 有三种架构: Encoder only, Decoder only, Encoder-Decoder.

### Encoder only

只有 Encoder 模块，一般用于语义理解，分析，将提取的上下文信息用于其他类型的任务。常用于模型预训练。

### Decoder only

只有 Decoder 模块，由于没有 Encoder 模块，所以 Decoder 模块中没有 Encoder-Decoder 注意力层。和 Encoder 模块的主要区别是：自注意力模块有 Mask。一般被用于问答等以生成为主的任务。

### Encoder-Decoder

具有 Encoder 和 Decoder 模块，常被用于输出高度依赖输入的任务。如：机器翻译，文字总结。

## 拆分维度

```py
def split(self, tensor):
  batch_size, length, d_model = tensor.size()

  d_tensor = d_model // self.n_head
  tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)

  return tensor
```

这里把输入的 `tensor` 改成了四轴的张量, 把 `d_model` 轴拆成了 `n_head` 和 `d_tensor` 两轴. 然后通过转置，令后两轴为 `length` 和 `d_tensor`。

每个头注意到的信息是不同的，但是注意到的总信息量是不变的。

拆分后对 `tensor` 的最后两轴做注意力模块, 最后 `concat` 回原来的 shape.

```py
def concat(self, tensor):
  batch_size, head, length, d_tensor = tensor.size()
  d_model = head * d_tensor

  tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)
  return tensor
```

将两轴转置回来。使得最后两轴表示 `n_head` 和 `d_tensor`, 然后合并为 `d_model` 轴.