# 核方法

## 核函数

衡量输入相似度的函数.

## 核矩阵

$K$.

$n$ 阶方阵, 元素是 $n$ 个输入的两两核函数值.

# 高斯过程

高斯分布:

$$
P(x;\mu, \sigma) = \frac{e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}
$$

升维, 将 $x$, $\mu$ 升维至 $n$ 维向量, $\Sigma$ 升维至 $n$ 阶协方差矩阵.

$$
P(x;\mu, \Sigma) = \frac{e^{-\frac 12 ((x-\mu)^T \sigma (x-\mu))}}{|\Sigma|{2\pi}^{\frac d2}}
$$

$|\Sigma|$ 表示协方差矩阵的行列式.

## 随机过程

随着某个变量（通常是时间或空间）变化的随机变量集合

高斯过程是随机过程.

**相似的 $x$, 就有相似的 $y$**

我们认为, 我们需要的函数 $f(x)$ 对于一些输入 $x_1, x_2, ..., x_n$, 相似的 $x$ 就会得到相似的 $f(x)$.

则 $f(x_1), f(x_2), ..., f(x_n) \~ N(\mu(x), K(x,x'))$, $f(x_i)$ 和 $f(x_j)$ 的协方差, 也就是分布的相似程度, 也就是 $K(x_i, x_j)$.

对于训练数据和测试数据, 我们把它们放在同一个分布里面:

$$
\begin{bmatrix}
\mathbf{f} \\
f_*
\end{bmatrix}
\sim \mathcal{N}\left(0,
\begin{bmatrix}
K(X,X) & K(X, x_*) \\
K(x_*, X) & K(x_*, x_*)
\end{bmatrix}
\right)
$$

这是先验分布, 每个 $y$ 的均值仍然是 $0$, 只是描述了它们关于 $x$ 的区别.

接下来, 将训练输出的信息加入到模型中, 相当于锚定了在观测点上的函数值, 从而约束了观测点附近的函数值分布.

$$
\mu_* = K(x_*, X)[K(X,X) + \sigma^2 I]^{-1} y
$$

$$
\sigma_*^2 = K(x_*, x_*) - K(x_*, X)[K(X,X) + \sigma^2 I]^{-1}K(X, x_*)
$$

$\mu_*$ 的预测里, $K(x_*, X)$ 表示这一条测试输入和每个训练输入的相似度, 后面的 $y$ 应该是所有训练输出, 这里就很像根据相似度加权的平均数. 中间的矩阵 $[K(X,X) + \sigma^2 I]^{-1}$ 是根据添加噪声后的训练数据的相关性, 简单来说, 作用是减少相似度特别高的训练数据的过多的加权.

$\sigma_*^2$ 的预测则是由两部分组合而成 $K(x_*, x_*)$ 是先验方差, 后面的部分则表示从训练数据中获得的信息量, 这部分信息量越大, 结果的不确定性就越小.

协方差矩阵 $\Sigma$ 具有这些性质:

-$\Sigma_{i,i} = 0$
-$\Sigma$ 半正定

# K-means

聚类, 最小化聚类内点的 MSE:

$$
Z(C_1, C_2, ..., C_k) = \sum_{l = 1}^k \frac C2 \sum_{i,j \in C_l} ||x_i - x_j||^2
$$

设每个聚类的质心为 $\mu_i$, 则 $Z$ 还可以写成:

$$
Z(C_1, C_2, ..., C_k) = \sum_{l = 1}^k \sum_{i \in C_l} ||x_i - \mu_l||^2
$$

## Lloyd 算法

每次计算所有聚类的均值, 将每个数据点分配到最近的均值所在的聚类.

达到预定迭代次数或收敛则停止.


