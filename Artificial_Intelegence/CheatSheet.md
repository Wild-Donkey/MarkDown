# 机器学习

监督学习: 我们知道正确的答案或期望的输出. 有分类和回归两种.
无监督学习: 我们不展示输出，或者达到特定输出所需的特定输入。例如: 聚类, 降维, 关联分析.
半监督学习: 同时使用有标签和无标签的数据进行训练.
自监督学习: 自动生成标签, 然后用生成的标签进行训练.

泛化 (Generalization): 模型适用于新样本的能力即泛化能力。一般而言训练样本越多越有可能通过学习获得强泛化能力的模型

假设空间 (Hypothesis)：对于当前任务，学习算法所考虑的所有可能的假设构成的空间称为假设空间，即模型可以表示的所有可能的解。

奥卡姆剃刀: 若非必要，勿增实体. 假设空间中可能有多个与训练集一致的假设，但他们对测试数据会预测出不同的结果. 学习过程中对某种类型假设的偏好称作归纳偏好. 若有多个假设与观察一致，选最简单的那个.

没有免费的午餐: 没有一种算法能在所有问题上都表现最好

概率近似正确: 学习算法能够以**高概率**在一个**接近正确**的程度上输出正确的假设.

参数模型: 强假设. 通常假设总体服从某个分布，该分布由一些参数确定。在此基础上构建的模型，有固定数量的参数。
非参数模型: 更灵活. 对于总体分布不做任何假设，只知道分布是存在的。非参数模型并不是没有参数，而是模型的参数量随着训练数据的增加而增长。

# 损失函数

## 0-1 损失

$$
\frac 1n \sum_{i} [\hat y_i \neq y_i]
$$

不连续和非凸的, 难以优化.

## 均方损失

$$
\frac 1n \sum_{i} (\hat y_i - y_i)^2
$$

损失非负, 平方增长.

## 绝对值损失

$$
\frac 1n \sum_{i} |\hat y_i - y_i|
$$

适合有噪声的数据, 这样错误数据不会像在均方损失里那样主导损失.

如果 $y$ 关于 $x$ 的概率分布是 $P(y|x)$, 则期望绝对值损失最小的预测就应当是概率中值 (中位数).

# 监督学习

## 混淆矩阵

||$\hat y = 1$| $\hat y = 0$ |
|-|-|-|
|$y = 1$|TP|FN| 
|$y = 0$|FP|TN| 

查准率: $\frac {TP}{TP + FP}$
查全率: $\frac {TP}{TP + FN}$

## 训练, 验证, 测试

有时候用测试集验证. (反正也不会从验证集学习)

按时间先后划分训练集和测试集可以让模型学习过去的模式来解决未来的问题. 否则最好均匀随机分割, 保证两个数据集的分布相同.

留出法: 训练集和测试集互斥
交叉验证法: 分成 $k$ 个互斥子集, 每次用一个子集做测试集, 剩下的 $k - 1$ 个做训练集.
自助法: 有放回随机抽样.

# k-NN

假设: 相似的输入有相似的输出.

对于一个 $x_*$ 在 $k$ 个最相似的 $x$ 中选择 (出现最多的标签/标签的平均值).

也可以按距离给 $k$ 个邻居加权.

$k$ 大了欠拟合, 小了过拟合.

## Minkowski 距离

$$
\left (\sum_{r = 1}^d |x_r - z_r|^p \right )^{\frac 1p}
$$

$p = 1$ 时, 即为曼哈顿距离, $p = 2$ 时即为欧氏距离, $p \to \infin$ 时即为切比雪夫距离.

## Hamming 距离

$$
\frac 1n \sum_{i} [x_i \neq y_i]
$$

即二进制串异或后 $1$ 的数量 (的均值).

## 余弦相似度

余弦越大, 指向越相同; 余弦趋于 $0$, 指向垂直; 余弦越小, 指向越相反.

$$
cos \left <A, B\right > = \frac {A * B}{|A||B|}
$$

## 贝叶斯最优分类器

假设知道了所有 $x$, $y$ 的分布 $P(y|x)$. 那么预测 $\hat y$ 就是 $\argmax_y P(y|x^*)$.

那么错误率就是当 $\hat y \neq y^x$ 的概率, 也就是 $1 - P(\hat y|x^*)$.

### 1-NN

当样本数 $n \to \infin$, 类别数 $m = 2$ 时, 1-NN 错误率不大于贝叶斯最优分类器的两倍.

由于 $n \to \infin$, $x^*$ 和 $x$ 同分布, 所以存在足够多的 $(x_i, y_i)$ 满足 $x_i = x^*$. 而且成为最近邻的机会, 即输出的分布有 $P(\hat y | x^*) = P(y^* | x^*)$.

1nn的错误率为: $P(y_0 | x^*)(1 - P(\hat y_0 | x^*)) + (1 - P(\hat y_1 | x^*))P(\hat y_1 | x^*)$, 也就是 $2P(y_0 | x^*)(1 - P(y_0 | x^*)) \leq 2P(y^* | x^*)$.

课件没说是二分类, 耽误了我好久.

## 维度灾难

维度特别大时, 对于随机分布于边长为 $1$ 的超立方体中的数据点, 包含一个点和它最近的 $k$ 个邻居的最小超立方体会越来越趋近于全体数据分布的大超立方体; 随机分布的点对之间的距离越来越趋近于一个由维度数决定的常数.

高维数据一般会先降维处理.

## 约简

类似于剪枝, 将数据中的*类离群点*删除, 剩余的数据分为原型和吸收点, 保留原型, 删除吸收点, 可以减小模型规模.

类离群点的寻找可以给出一个阈值 $r < k$, 如果一个点的 $k$ 近邻中有大于等于 $r$ 个和它不在同一个类别, 则删除之.

# 核方法

## 核函数

衡量输入相似度的函数.

## 核矩阵

$K$.

$n$ 阶方阵, 元素是 $n$ 个输入的两两核函数值.

# 高斯过程

高斯分布:

$$
P(x;\mu, \sigma) = \frac{e^{-\frac {(x-\mu)^2}{2\sigma^2}}}{\sigma\sqrt{2\pi}}
$$

升维, 将 $x$, $\mu$ 升维至 $n$ 维向量, $\Sigma$ 升维至 $n$ 阶协方差矩阵.

$$
P(x;\mu, \Sigma) = \frac{e^{-\frac 12 ((x-\mu)^T \sigma (x-\mu))}}{|\Sigma|{2\pi}^{\frac d2}}
$$

$|\Sigma|$ 表示协方差矩阵的行列式.

## 随机过程

随着某个变量（通常是时间或空间）变化的随机变量集合

高斯过程是随机过程.

**相似的 $x$, 就有相似的 $y$**

我们认为, 我们需要的函数 $f(x)$ 对于一些输入 $x_1, x_2, ..., x_n$, 相似的 $x$ 就会得到相似的 $f(x)$.

则 $f(x_1), f(x_2), ..., f(x_n) \~ N(\mu(x), K(x,x'))$, $f(x_i)$ 和 $f(x_j)$ 的协方差, 也就是分布的相似程度, 也就是 $K(x_i, x_j)$.

对于训练数据和测试数据, 我们把它们放在同一个分布里面:

$$
\begin{bmatrix}
\mathbf{f} \\
f_*
\end{bmatrix}
\sim \mathcal{N}\left(0,
\begin{bmatrix}
K(X,X) & K(X, x_*) \\
K(x_*, X) & K(x_*, x_*)
\end{bmatrix}
\right)
$$

这是先验分布, 每个 $y$ 的均值仍然是 $0$, 只是描述了它们关于 $x$ 的区别.

接下来, 将训练输出的信息加入到模型中, 相当于锚定了在观测点上的函数值, 从而约束了观测点附近的函数值分布.

$$
\mu_* = K(x_*, X)[K(X,X) + \sigma^2 I]^{-1} y
$$

$$
\sigma_*^2 = K(x_*, x_*) - K(x_*, X)[K(X,X) + \sigma^2 I]^{-1}K(X, x_*)
$$

$\mu_*$ 的预测里, $K(x_*, X)$ 表示这一条测试输入和每个训练输入的相似度, 后面的 $y$ 应该是所有训练输出, 这里就很像根据相似度加权的平均数. 中间的矩阵 $[K(X,X) + \sigma^2 I]^{-1}$ 是根据添加噪声后的训练数据的相关性, 简单来说, 作用是减少相似度特别高的训练数据的过多的加权.

$\sigma_*^2$ 的预测则是由两部分组合而成 $K(x_*, x_*)$ 是先验方差, 后面的部分则表示从训练数据中获得的信息量, 这部分信息量越大, 结果的不确定性就越小.

协方差矩阵 $\Sigma$ 具有这些性质:

-$\Sigma_{i,i} = 0$
-$\Sigma$ 半正定

# K-means

聚类, 最小化聚类内点的 MSE:

$$
Z(C_1, C_2, ..., C_k) = \sum_{l = 1}^k \frac C2 \sum_{i,j \in C_l} ||x_i - x_j||^2
$$

设每个聚类的质心为 $\mu_i$, 则 $Z$ 还可以写成:

$$
Z(C_1, C_2, ..., C_k) = \sum_{l = 1}^k \sum_{i \in C_l} ||x_i - \mu_l||^2
$$

## Lloyd 算法

每次计算所有聚类的均值, 将每个数据点分配到最近的均值所在的聚类.

达到预定迭代次数或收敛则停止.


